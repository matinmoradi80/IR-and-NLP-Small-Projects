{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین دوم</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ... آبان <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1> \n",
    "مقدمه\n",
    "</h1>\n",
    "<p>\n",
    "در این تمرین قصد داریم به مباحث زیر بپردازیم:\n",
    "    <li>مدل‌های برداری</li>\n",
    "    <li>امتیازدهی و ارزیابی سیستم بازیابی</li>\n",
    "    <li>مدل‌های احتمالاتی</li>\n",
    "\n",
    "دیتاست مورد استفاده در این تمرین را می‌توانید در کنار این فایل مشاهده کنید. همچنین لطفا پس از اتمام تمرین یک بار از اول تا آخر نوت‌بوک را اجرا کنید تا مطمئن باشید تمام سل‌ها به درستی کار می‌کنند. \n",
    "\n",
    "کتابخانه‌های مورد نظرتان را هم می‌توانید در اولین سل نوت‌بوک فراخوانی کنید. \n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "import math\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from collections import defaultdict\r\n",
    "from typing import List, Union\r\n",
    "import math\r\n",
    "import json\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1>1.\n",
    "آماده‌سازی دیتاست\n",
    "</h1>\n",
    "<p>\n",
    "با استفاده از تمرین قبل و عملیات‌هایی که در آنجا برای پیش‌پردازش متون پیاده‌سازی کردید، دیتاست داده شده را لود کنید. قرار است در ادامه با این دیتاست بخش‌های بعدی تمرین را پیاده‌سازی کنید. همچنین در صورتی که اجرای سل‌های بعدی برایتان طول کشید، می‌توانید آن را کوچک کنید. مثلا از ۷۰درصد دیتای آن استفاده کنید. \n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "file_path = 'data.csv'\r\n",
    "df_all = pd.read_csv(file_path)\r\n",
    "\r\n",
    "df = df_all[df_all['abstract'].notna()]\r\n",
    "df.reset_index(drop=True, inplace=True)\r\n",
    "df = df.drop(df.columns[-2:], axis=1)\r\n",
    "\r\n",
    "print(df.info())\r\n",
    "print()\r\n",
    "\r\n",
    "if df['title'].isna().any():\r\n",
    "    print(\"There are empty cells in the column 'title'.\")\r\n",
    "else:\r\n",
    "    print(\"There's no empty cell in the column 'title'.\")\r\n",
    "    \r\n",
    "print(df[100:105])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3939 entries, 0 to 3938\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   paperId   3939 non-null   object\n",
      " 1   title     3939 non-null   object\n",
      " 2   abstract  3939 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 92.4+ KB\n",
      "None\n",
      "\n",
      "There's no empty cell in the column 'title'.\n",
      "                                      paperId  \\\n",
      "100  3f271912e46c926555a9078511ed681b58a5c27d   \n",
      "101  d3201039430a7fdce2b27cf46a54531d2731de6c   \n",
      "102  2f56d4ef0a9df1672699ecd787ffc9b3996026e2   \n",
      "103  aeb64039aea4e681204c96fb76b2398d45dc430b   \n",
      "104  1a8523be5aa3f91877c831660410be7fd2e2bd00   \n",
      "\n",
      "                                                 title  \\\n",
      "100  Deep Convolutional Auto-encoders Based Fault D...   \n",
      "101  Anisotropic tough multilayer hydrogels with pr...   \n",
      "102  Learning retrosynthetic planning through self-...   \n",
      "103  Packet Video Error Concealment With Gaussian M...   \n",
      "104  Using pulsed gradient spin echo NMR for chemic...   \n",
      "\n",
      "                                              abstract  \n",
      "100  As a kind of power supply equipment, diesel ge...  \n",
      "101  This study presents a novel self-welding-based...  \n",
      "102  The problem of retrosynthetic planning can be ...  \n",
      "103  In this paper, Gaussian mixture modeling is ap...  \n",
      "104  Pulsed gradient spin echo NMR is a powerful te...  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import string\r\n",
    "\r\n",
    "nltk.download('stopwords')\r\n",
    "print('Stopwords:')\r\n",
    "print(stopwords.words('english'))\r\n",
    "print('\\nPunctuations:')\r\n",
    "print(string.punctuation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuations:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "# %%capture\r\n",
    "\r\n",
    "# !pip install spacy\r\n",
    "# !python -m spacy download en_core_web_sm\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "import string\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "\r\n",
    "class Preprocessor:\r\n",
    "\r\n",
    "    def __init__(self, given_stopwords):\r\n",
    "        self.stopwords = given_stopwords\r\n",
    "\r\n",
    "    def preprocess(self, text):\r\n",
    "        text = self.remove_links_and_tags(text)\r\n",
    "        words = self.word_tokenize(text)\r\n",
    "        stopwords_removed_words = self.remove_stopwords(words)\r\n",
    "        punctuation_removed_words = self.remove_punctuations(stopwords_removed_words)\r\n",
    "        final_words = self.normalize(punctuation_removed_words)\r\n",
    "        return final_words      \r\n",
    "\r\n",
    "    def remove_links_and_tags(self, text):\r\n",
    "        link_removed_text = re.sub(r'http\\S+', '', text)\r\n",
    "        clean_text = re.sub(r'<.*?>', '', link_removed_text)\r\n",
    "        return clean_text\r\n",
    "\r\n",
    "    def word_tokenize(self, text):\r\n",
    "        words = word_tokenize(text)\r\n",
    "        return words\r\n",
    "\r\n",
    "    def remove_stopwords(self, words):\r\n",
    "        stopwords_collection = self.stopwords + stopwords.words('english')\r\n",
    "        words = [word for word in words if word not in stopwords_collection]\r\n",
    "        return words\r\n",
    "\r\n",
    "    def remove_punctuations(self, words):\r\n",
    "        punctuaction_removed_words = [''.join([char for char in word if char not in string.punctuation]) for word in words]\r\n",
    "        return punctuaction_removed_words\r\n",
    "\r\n",
    "    def normalize(self, words):\r\n",
    "        doc = nlp(' '.join(words))\r\n",
    "        normalized_words = [token.lemma_.lower() for token in doc if len(token.text) > 2 and not token.is_space]\r\n",
    "        return normalized_words\r\n",
    "\r\n",
    "        # lemmatizer = WordNetLemmatizer()\r\n",
    "        # normalized_words=[]\r\n",
    "        # for word in words:\r\n",
    "        #     if len(word) > 2:\r\n",
    "        #         normalized_words.append(lemmatizer.lemmatize(word.lower()))\r\n",
    "        # return normalized_words\r\n",
    "        # NOTE: NLTK Lemmatizer was not performing very good. So I used Spacy instead which is stronger"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "text = \"\"\"An NoC (network on chip) constructed with silicon photonic microrings can be implemented with CMOS technology and integrated with processor cores on the same die. Each microring in the NoC has two switching states, one consuming significantly less power than the other. Different routing schemes can lead to different numbers of microrings in the high-power consumption state, and result in different power consumptions for the NoC. In an earlier work, a looping-based routing algorithm was proposed which exploits the different power consumption characteristics of the two switching states of a microring so as to minimize the power consumption of a Benes-type NoC; but this algorithm cannot provide an optical solution. In this paper, we will show the necessary and sufficient conditions for finding the optimal solution for a Benes-type NoC. We also present a new routing algorithm. Compared with the algorithm of the previous work, the new algorithm can reduce the number of microrings in the high-power-consumption state by as many as <inline-formula> <tex-math notation=\"LaTeX\">$(n - 2)\\cdot 2^{n-2}$ </tex-math></inline-formula> in a <inline-formula> <tex-math notation=\"LaTeX\">$2^{n}\\times 2^{n}$ </tex-math></inline-formula> Benes-type NoC for certain interconnection patterns.\"\"\"\r\n",
    "preprocessor = Preprocessor([])\r\n",
    "cleaned_words = preprocessor.preprocess(text)\r\n",
    "print(cleaned_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['noc', 'network', 'chip', 'construct', 'silicon', 'photonic', 'microring', 'implement', 'cmos', 'technology', 'integrate', 'processor', 'core', 'die', 'each', 'microre', 'noc', 'two', 'switch', 'state', 'one', 'consume', 'significantly', 'less', 'power', 'different', 'routing', 'scheme', 'lead', 'different', 'number', 'microring', 'highpower', 'consumption', 'state', 'result', 'different', 'power', 'consumption', 'noc', 'early', 'work', 'loopingbase', 'routing', 'algorithm', 'propose', 'exploit', 'different', 'power', 'consumption', 'characteristic', 'two', 'switch', 'state', 'microre', 'minimize', 'power', 'consumption', 'benestype', 'noc', 'algorithm', 'provide', 'optical', 'solution', 'paper', 'show', 'necessary', 'sufficient', 'condition', 'find', 'optimal', 'solution', 'benestype', 'noc', 'also', 'present', 'new', 'routing', 'algorithm', 'compare', 'algorithm', 'previous', 'work', 'new', 'algorithm', 'reduce', 'number', 'microring', 'highpowerconsumption', 'state', 'many', 'cdot', 'ime', 'benestype', 'noc', 'certain', 'interconnection', 'pattern']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "for index, value in df.iterrows():\r\n",
    "    df.at[index, 'title'] = preprocessor.preprocess(value['title'])\r\n",
    "    if pd.notna(df.at[index, 'abstract']):\r\n",
    "        df.at[index, 'abstract'] = preprocessor.preprocess(value['abstract'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "df['title'][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['variational', 'model', 'disocclusion']"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h2>1-1.\r\n",
    "نمایه‌سازی\r\n",
    "</h2>\r\n",
    "<p>\r\n",
    "در این بخش باید برای سامانه یک\r\n",
    "Positional Index\r\n",
    "بسازید. \r\n",
    "<br>\r\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا چکیده آن، در این قسمت باید نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید.\r\n",
    "با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره‌ی تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه‌ی جایگاه‌های این کلمه در هر بخش از هر سند را پیدا کرد.\r\n",
    "\r\n",
    "برای این بخش می‌توانید از نمایه‌ای که در تمرین اول زدید استفاده کنید. \r\n",
    "</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "def construct_positional_indexes(corpus: df) -> dict:\r\n",
    "    \"\"\"\r\n",
    "    Constructs positional indexes for the processed data by inserting words into a trie and creating positional indexes and posting lists.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    positional_index = defaultdict(lambda: defaultdict(lambda: {'title': [], 'abstract': []}))\r\n",
    "    for index, row in corpus.iterrows():\r\n",
    "        paper_id = row['paperId']\r\n",
    "        for position, token in enumerate(row['title']):\r\n",
    "            positional_index[token][paper_id]['title'].append(position)\r\n",
    "\r\n",
    "        for position, token in enumerate(row['abstract']):\r\n",
    "            positional_index[token][paper_id]['abstract'].append(position)\r\n",
    "\r\n",
    "    return positional_index    \r\n",
    "\r\n",
    "docs = construct_positional_indexes(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "docs['network']['cf379f63c71160875187c2044df8e18b5627fc7b']['abstract']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[55, 123]"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "row = df[df['paperId']=='597c2d96c45e8ad83fc08e5d464d266b68f873ed']\r\n",
    "print(row['abstract'].values[0])\r\n",
    "print(row['title'].values[0])\r\n",
    "\r\n",
    "docs['stomatitis']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'emergence', 'sarscov2', 'create', 'need', 'robust', 'assay', 'plasma', 'monoclonal', 'antibody', 'neutralization', 'potency', 'pseudotyped', 'hiv1', 'vesicular', 'stomatitis', 'virus', 'base', 'reporter', 'virus', 'replicationcompetent', 'vesicular', 'stomatitis', 'virussarscov2', 'chimera', 'represent', 'useful', 'tool', 'assess', 'neutralizing', 'antibody']\n",
      "['measure', 'sarscov2', 'neutralize', 'antibody', 'activity', 'use', 'pseudotype', 'chimeric', 'virus']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.construct_positional_indexes.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "            {'597c2d96c45e8ad83fc08e5d464d266b68f873ed': {'title': [],\n",
       "              'abstract': [15, 22]},\n",
       "             '86d3e6c0d3c9856f36bbe0a51f4755505dbbc487': {'title': [],\n",
       "              'abstract': [152]},\n",
       "             '0b9add5afcd6ceb05a846ed2b8eb7d566e6cc3cf': {'title': [],\n",
       "              'abstract': [55]}})"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1>2.\n",
    "مدل‌های برداری و احتمالاتی\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش قصد داریم تا با استفاده از مدل‌های برداری و احتمالاتی، دو سیستم بازیابی اطلاعات طراحی کنیم. در نهایت قرار است سیستم‌های طراحی شده در این بخش را مورد ارزیابی قرار دهیم.\n",
    "\n",
    "در مدل‌های برداری ما به ازای هر داک که در اختیار داریم و کوئری ورودی یک بردار در فضا در نظر می‌گیریم. در ادامه برای بدست آوردن میزان ارتباط داک‌ها و کوئری، از معیارهایی مانند ضرب داخلی بردارها استفاده می‌کنیم. در این بخش ابتدا بردارهای مربوط به هرکدام را با استفاده از مقادیر \n",
    "tf, idf\n",
    "می‌سازیم و سپس به سراغ محاسبه امتیاز داک‌ها می‌رویم تا در نهایت بتوانیم با استفاده از این امتیازها داک‌هایی مرتبط‌تر را خروجی دهیم و یک سیستم بازیابی کامل را طراحی کنیم. \n",
    "\n",
    "در مدل‌های احتمالاتی اساس کار محاسبه \n",
    "$P(R | d, q)$\n",
    "است. در این روش می‌خواهیم به نوعی داک‌ها را به صورت مدل‌های احتمالاتی ببینیم. در درس مشاهده کردید که در انتها مدل‌های احتمالاتی هم رفتاری مشابه با مدل‌های برداری دارند. در ادامه یک سیستم مبتنی بر این مدل‌ها و با روش \n",
    "Okapi25\n",
    "طراحی می‌کنید. \n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h2>2-1.\r\n",
    "ساخت ماتریس TF-IDF\r\n",
    "</h2>\r\n",
    "<p>\r\n",
    "در این مرحله دو تابع\r\n",
    "<code dir=\"ltr\">get_tf(token, doc_id)</code>\r\n",
    "و\r\n",
    "<code dir=\"ltr\">get_idf(token)</code>\r\n",
    "را پیاده‌سازی کنید که تابع اول مقدار\r\n",
    "tf\r\n",
    "توکن ورودی را در شناسه‌ی موردنظر\r\n",
    "و تابع دوم مقدار\r\n",
    "idf\r\n",
    "توکن ورودی را برروی نمایه‌ی کنونی حساب می‌کند و آن را به عنوان خروجی برمی‌گرداند.\r\n",
    "</p>\r\n",
    "</div>\r\n",
    "\r\n",
    "$$tf_{t, d} = \\text{Numbers of } t \\text{ in the title of document } d + \\text{Numbers of } t \\text{ in the abstract of document } d$$\r\n",
    "$$idf_t = \\log\\Biggl(\\frac{\\text{Number of documents}}{\\text{Number of documents that contains } t + 1}\\Biggr)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "def get_tf(token: str, doc_id: str) -> int:\r\n",
    "    \"\"\"\r\n",
    "    Retrieves the term frequency (TF) of a given token within a specific document.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    token : str\r\n",
    "        The token for which to retrieve the term frequency.\r\n",
    "    doc_id : str\r\n",
    "        The unique identifier of the document in which to calculate the TF.\r\n",
    "\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    int\r\n",
    "        The term frequency (TF) of the given token within the specified document.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    tf = len(docs[token][doc_id]['title']) + len(docs[token][doc_id]['abstract'])\r\n",
    "    return tf\r\n",
    "\r\n",
    "get_tf('network', 'cf379f63c71160875187c2044df8e18b5627fc7b')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "def get_idf(token: str) -> float:\r\n",
    "    \"\"\"\r\n",
    "    Retrieves the inverse document frequency (IDF) of a given token.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    token : str\r\n",
    "        The token to retrieve the IDF for.\r\n",
    "\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    float\r\n",
    "        The IDF of the given token.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    number_of_documents = len(df['paperId'])\r\n",
    "    docs_having_token = len(docs[token])\r\n",
    "    result_base_10 = math.log10(number_of_documents / (docs_having_token+1))\r\n",
    "    return result_base_10\r\n",
    "\r\n",
    "print(f\"'deep' idf: {get_idf('deep')}\")\r\n",
    "print(f\"'psychiatric' idf: {get_idf('psychiatric')}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'deep' idf: 0.9783856396882428\n",
      "'psychiatric' idf: 2.750287940794885\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<p>\r\n",
    "در مرحله‌ی نهایی تابع\r\n",
    "<code dir=\"ltr\">generate_tfidf_list(corpus, positional_index)</code>\r\n",
    "را پیاده‌سازی کنید که با ورودی گرفتن\r\n",
    "corpus\r\n",
    "و\r\n",
    "positional_index\r\n",
    "مقادیر\r\n",
    "tf-idf\r\n",
    "را برای هر\r\n",
    "token\r\n",
    "به دست می‌آورد و به شکل یک ماتریس به عنوان خروجی برمی‌گرداند.\r\n",
    "</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "def generate_tfidf_list(corpus: df, positional_index: dict) -> list[dict[str, float]]:\r\n",
    "    \"\"\"\r\n",
    "    Generates a list of dictionaries representing documents with associated TF-IDF scores.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    corpus : dict\r\n",
    "        The corpus containing the processed data.\r\n",
    "    positional_index : dict\r\n",
    "        The positional index containing the term frequencies and document frequencies.\r\n",
    "\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    list[dict[str, float]]\r\n",
    "        The list of dictionaries representing documents with associated TF-IDF scores.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    idf_values = {token: get_idf(token) for token in positional_index}\r\n",
    "    tfidf_matrix = []\r\n",
    "\r\n",
    "    print('Index of the Docs that are processed:')\r\n",
    "    for i, row in corpus.iterrows():\r\n",
    "        print('  ', i, end=\"\")\r\n",
    "        tfidf_dict = {token: get_tf(token, row['paperId']) * idf_values[token] for token in positional_index}\r\n",
    "        tfidf_matrix.append(tfidf_dict)\r\n",
    "    return tfidf_matrix    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "#Since the matrix of alldocs-alltokens is too big, it was taking toooo much time to build it. So here I will beak it down,\r\n",
    "#so that we can process it separatly and build smaller matrixes. Here I did it for the first part for instance:\r\n",
    "total_rows = len(df)\r\n",
    "rows_per_df = total_rows // 7\r\n",
    "\r\n",
    "df1 = df.iloc[:rows_per_df]\r\n",
    "df2 = df.iloc[rows_per_df: 2 * rows_per_df]\r\n",
    "df3 = df.iloc[2 * rows_per_df: 3 * rows_per_df]\r\n",
    "df4 = df.iloc[3 * rows_per_df: 4 * rows_per_df]\r\n",
    "df5 = df.iloc[4 * rows_per_df: 5 * rows_per_df]\r\n",
    "df6 = df.iloc[5 * rows_per_df: 6 * rows_per_df]\r\n",
    "df7 = df.iloc[6 * rows_per_df:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "print('first df:')\r\n",
    "tfidf_matrix1 = generate_tfidf_list(df1, docs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "first df:\n",
      "Index of the Docs that are processed:\n",
      "   0   1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96   97   98   99   100   101   102   103   104   105   106   107   108   109   110   111   112   113   114   115   116   117   118   119   120   121   122   123   124   125   126   127   128   129   130   131   132   133   134   135   136   137   138   139   140   141   142   143   144   145   146   147   148   149   150   151   152   153   154   155   156   157   158   159   160   161   162   163   164   165   166   167   168   169   170   171   172   173   174   175   176   177   178   179   180   181   182   183   184   185   186   187   188   189   190   191   192   193   194   195   196   197   198   199   200   201   202   203   204   205   206   207   208   209   210   211   212   213   214   215   216   217   218   219   220   221   222   223   224   225   226   227   228   229   230   231   232   233   234   235   236   237   238   239   240   241   242   243   244   245   246   247   248   249   250   251   252   253   254   255   256   257   258   259   260   261   262   263   264   265   266   267   268   269   270   271   272   273   274   275   276   277   278   279   280   281   282   283   284   285   286   287   288   289   290   291   292   293   294   295   296   297   298   299   300   301   302   303   304   305   306   307   308   309   310   311   312   313   314   315   316   317   318   319   320   321   322   323   324   325   326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   350   351   352   353   354   355   356   357   358   359   360   361   362   363   364   365   366   367   368   369   370   371   372   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398   399   400   401   402   403   404   405   406   407   408   409   410   411   412   413   414   415   416   417   418   419   420   421   422   423   424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   450   451   452   453   454   455   456   457   458   459   460   461   462   463   464   465   466   467   468   469   470   471   472   473   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499   500   501   502   503   504   505   506   507   508   509   510   511   512   513   514   515   516   517   518   519   520   521   522   523   524   525   526   527   528   529   530   531   532   533   534   535   536   537   538   539   540   541   542   543   544   545   546   547   548   549   550   551   552   553   554   555   556   557   558   559   560   561"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "# print('second df:')\r\n",
    "# tfidf_matrix2 = generate_tfidf_list(df2, docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "# print('third df:')\r\n",
    "# tfidf_matrix3 = generate_tfidf_list(df3, docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "# print('forth df:')\r\n",
    "# tfidf_matrix4 = generate_tfidf_list(df4, docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "def show_document_vector(vector: list[dict[str, float]], doc_index: int) -> None:\r\n",
    "    \"\"\"\r\n",
    "    Prints the non-zero weights and corresponding terms for a document represented as a vector.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    vector : list of dict of str:float\r\n",
    "        The list of document vectors, where each document vector is a dictionary with term weights.\r\n",
    "    doc_id : int\r\n",
    "        The document index for which to display the vector.\r\n",
    "\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    None\r\n",
    "    \"\"\"\r\n",
    "    vector_list = [(term, weight) for term, weight in vector[doc_index].items() if weight > 0]\r\n",
    "    vector_list.sort(key=lambda x: x[1], reverse=True)\r\n",
    "    length = math.sqrt(sum(weight ** 2 for _, weight in vector_list))\r\n",
    "    normalized = {term: tfidf / length for term, tfidf in vector_list}\r\n",
    "    for term, tfidf in vector_list:\r\n",
    "        print(f'{term.ljust(16)}\\t{tfidf}\\t(normalized: {normalized[term]})')\r\n",
    "\r\n",
    "show_document_vector(tfidf_matrix1, 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stomatitis      \t5.9866519789623585\t(normalized: 0.3856087860694277)\n",
      "vesicular       \t5.792831952946246\t(normalized: 0.3731245619637579)\n",
      "antibody        \t5.109874134355984\t(normalized: 0.32913427552508273)\n",
      "virus           \t4.4048435473330025\t(normalized: 0.28372225061381184)\n",
      "pseudotyped     \t3.2943559851451605\t(normalized: 0.2121941640797634)\n",
      "replicationcompetent\t3.2943559851451605\t(normalized: 0.2121941640797634)\n",
      "virussarscov2   \t3.2943559851451605\t(normalized: 0.2121941640797634)\n",
      "pseudotype      \t2.9933259894811792\t(normalized: 0.19280439303471386)\n",
      "sarscov2        \t2.9047423611100935\t(normalized: 0.18709859528300954)\n",
      "chimera         \t2.8172347304254983\t(normalized: 0.18146210407579103)\n",
      "chimeric        \t2.750287940794885\t(normalized: 0.1771499659439231)\n",
      "hiv1            \t2.692295993817198\t(normalized: 0.1734146219896643)\n",
      "neutralizing    \t2.4192947217534604\t(normalized: 0.15583022097790653)\n",
      "neutralization  \t2.364937059430868\t(normalized: 0.1523289664778212)\n",
      "reporter        \t2.2943559851451605\t(normalized: 0.14778273889177984)\n",
      "potency         \t2.233658144791549\t(normalized: 0.14387310448877103)\n",
      "neutralize      \t2.1974459721371042\t(normalized: 0.14154062683894492)\n",
      "monoclonal      \t2.1640222166501544\t(normalized: 0.13938775511289578)\n",
      "emergence       \t1.8878158047112055\t(normalized: 0.1215969064738488)\n",
      "plasma          \t1.7032913781186614\t(normalized: 0.10971142517502758)\n",
      "assay           \t1.4618470724389243\t(normalized: 0.09415965334267257)\n",
      "create          \t1.4555068944079053\t(normalized: 0.09375127343974908)\n",
      "useful          \t1.4221997123968677\t(normalized: 0.09160591037742259)\n",
      "robust          \t1.247081117760981\t(normalized: 0.08032627212000835)\n",
      "represent       \t1.2432034626977793\t(normalized: 0.08007650683100012)\n",
      "assess          \t1.167251186780353\t(normalized: 0.07518431249288773)\n",
      "measure         \t1.1240942697502032\t(normalized: 0.07240451396025623)\n",
      "tool            \t1.1153790378519912\t(normalized: 0.07184315345284863)\n",
      "activity        \t1.0178941809719164\t(normalized: 0.0655640148869564)\n",
      "need            \t1.0144610051335226\t(normalized: 0.0653428791382845)\n",
      "base            \t0.5758542962778863\t(normalized: 0.037091595923882)\n",
      "use             \t0.24127754166174084\t(normalized: 0.015541030324285807)\n",
      "the             \t0.1896981941363642\t(normalized: 0.012218731039910006)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h2>2-2.\r\n",
    "امتیازدهی به سندها\r\n",
    "</h2>\r\n",
    "<p>\r\n",
    "در این بخش می‌خواهیم تا با سه شیوه‌ی متفاوت، داکیومنت‌ها را امتیازدهی کرده و kتا با بالاترین امتیاز را برگردانیم.\r\n",
    "روش‌ها به شکل زیر است:\r\n",
    "\r\n",
    "- **ltc.lnc:** در این روش از cosine similarity استفاده می‌کنیم و داده‌ها را بر اساس آن مرتّب می‌کنیم. برای متوجّه شدن نحوه‌ی عملکرد این سیستم، می‌توانید به اسلایدهای درس مراجعه کنید. همچنین دقت کنید که لازم است این امتیازدهی را هم به روش \r\n",
    "term-at-a-time\r\n",
    "و هم به روش \r\n",
    "doc-at-a-time\r\n",
    "محاسبه کنید.\r\n",
    "\r\n",
    "- **Okapi25:** برای پیاده‌سازی این روش از تساوی زیر استفاده کنید:\r\n",
    "</p>\r\n",
    "</div>\r\n",
    "<div>\r\n",
    "<p>\r\n",
    "\r\n",
    "$$ RSV_d = \\sum_{t \\in q} idf(t)\\times\\frac{(k_1 + 1)tf(t, d)}{k_1((1 - b) + b\\frac{dl(d)}{avg(dl)}) + tf(t, d)} $$\r\n",
    "</p>\r\n",
    "</div>\r\n",
    "<div dir='rtl'>\r\n",
    "<p>\r\n",
    "برای محاسبه این دو معیار، از توابعی که در بخش قبل پیاده‌سازی کردید استفاده کنید.\r\n",
    "\r\n",
    "در تابع search که مربوط به جست و جوی پرسمان کلی است، به عنوان ورودی پارامتر پرسمان (query)، روش محاسبه امتیاز (method)، تعداد اسنادی که باید برگردانده شود (max-result-count) را ورودی می گیرید. ورودی وزن \r\n",
    "(weight)\r\n",
    "نشان می‌دهد که امتیاز نهایی چه وزنی از امتیاز عنوان و چکیده دارد. به زبان دیگر:\r\n",
    "</p>\r\n",
    "</div>\r\n",
    "<div>\r\n",
    "<p>\r\n",
    "\r\n",
    "$$ final\\_score = weight \\times title\\_score + (1 - weight) \\times abstract\\_score $$\r\n",
    "</p>\r\n",
    "</div>\r\n",
    "<div dir='rtl'>\r\n",
    "<p>\r\n",
    "\r\n",
    "**توجّه کنید** که تابع نوشته شده، صرفاً یک prototype است و تا وقتی که نیازمندی‌های گفته شده را پیاده کنید، تغییر تابع مشکلی ندارد.\r\n",
    "</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "# Here's the code when there was no title_score_weight and I could simply use the methods I previously defined !!!!\r\n",
    "\r\n",
    "# def ltc_lnc_tat(query: str, max_result_count: int, title_score_weight: float = 0.1) :\r\n",
    "#     query_terms = preprocessor.preprocess(query)\r\n",
    "#     number_of_documents = len(df)\r\n",
    "#     scores = {}\r\n",
    "#     for term in query_terms:\r\n",
    "#         weight_t_q = 1 + math.log10(query_terms.count(term))    \r\n",
    "#         for paperId, title_abstract_dict  in docs[term].items():\r\n",
    "#             weight_t_d = (1 + math.log10(get_tf(term, paperId))) * get_idf(term)\r\n",
    "#             scores[paperId] = weight_t_d * weight_t_q\r\n",
    "#     sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\r\n",
    "#     print(sorted_scores)        \r\n",
    "\r\n",
    "\r\n",
    "# ltc_lnc_tat('oligonucleotide',  1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "def ltc_lnc_tat(query: str, max_result_count: int, title_score_weight: float = 0.5) :\r\n",
    "    query_terms = preprocessor.preprocess(query)\r\n",
    "    number_of_documents = len(df)\r\n",
    "    scores = {}\r\n",
    "    docs_normalization_factors ={}\r\n",
    "    \r\n",
    "    for _, row in df.iterrows():\r\n",
    "        scores[row['paperId']] = 0\r\n",
    "        docs_normalization_factors[row['paperId']] = 0\r\n",
    "\r\n",
    "    for term in query_terms:\r\n",
    "        weight_t_q = 1 + math.log10(query_terms.count(term))\r\n",
    "          \r\n",
    "        df_in_title = 0\r\n",
    "        df_in_abstract = 0\r\n",
    "        for paperId, title_abstract_dictionary in docs[term].items():\r\n",
    "            df_in_title += 1 if len(title_abstract_dictionary['title'])!=0 else 0\r\n",
    "            df_in_abstract += 1 if len(title_abstract_dictionary['abstract'])!=0 else 0\r\n",
    "        number_of_documents = len(df['paperId'])\r\n",
    "        idf_in_title = math.log10(number_of_documents / df_in_title) if df_in_title!=0 else 0  \r\n",
    "        idf_in_abstract = math.log10(number_of_documents / df_in_abstract) if df_in_abstract!=0 else 0\r\n",
    "        \r\n",
    "        for paperId, title_abstract_dict in docs[term].items():\r\n",
    "            tf_in_title = len(docs[term][paperId]['title'])\r\n",
    "            tf_in_abstract = len(docs[term][paperId]['abstract'])\r\n",
    "            title_score = (1 + math.log10(tf_in_title)) * idf_in_title if tf_in_title!=0 else 0\r\n",
    "            abstract_score = (1 + math.log10(tf_in_abstract)) * idf_in_abstract if tf_in_abstract!=0 else 0\r\n",
    "            weight_t_d = (title_score_weight * title_score) + (1-title_score_weight)*abstract_score\r\n",
    "            scores[paperId] += weight_t_d * weight_t_q\r\n",
    "\r\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\r\n",
    "    if max_result_count==-1:\r\n",
    "        return(sorted_scores)\r\n",
    "    else:    \r\n",
    "        return(dict(list(sorted_scores.items())[:max_result_count]))        \r\n",
    "\r\n",
    "\r\n",
    "ltc_lnc_tat('oligonucleotide nonhuman',  5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'65ae2070c175657121782e39d19adb499b165e73': 5.85015134293049,\n",
       " 'd8dd489d9b9d9c7bfef33b7da8e065315c184039': 3.48508681297258,\n",
       " '740058feedddff19f1af0b7c0cc066bb6245ea04': 3.007340351281301,\n",
       " 'bea51a1116b12f2b9acaf4f50280fcb854473f9b': 2.8428109916491886,\n",
       " '47c5f68c0c508323f5cfbd90bfd6c46e3606c75d': 2.8428109916491886}"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "def ltc_lnc_dat(query: str, max_result_count: int, title_score_weight: float = 0.5) :\r\n",
    "    query_terms = preprocessor.preprocess(query)\r\n",
    "    number_of_documents = len(df)\r\n",
    "    scores = {}\r\n",
    "    idf_in_title = {}\r\n",
    "    idf_in_abstract = {}\r\n",
    "    weight_t_q = 0\r\n",
    "\r\n",
    "    for term in query_terms:\r\n",
    "        weight_t_q = 1 + math.log10(query_terms.count(term))\r\n",
    "        df_in_title , df_in_abstract = 0, 0\r\n",
    "        for paperId, title_abstract_dictionary in docs[term].items():\r\n",
    "            df_in_title += 1 if len(title_abstract_dictionary['title'])!=0 else 0\r\n",
    "            df_in_abstract += 1 if len(title_abstract_dictionary['abstract'])!=0 else 0\r\n",
    "        number_of_documents = len(df['paperId'])\r\n",
    "        idf_in_title[term] = math.log10(number_of_documents / df_in_title) if df_in_title!=0 else 0  \r\n",
    "        idf_in_abstract[term] = math.log10(number_of_documents / df_in_abstract) if df_in_abstract!=0 else 0 \r\n",
    "\r\n",
    "    for i, row in df.iterrows():\r\n",
    "        paperId = row['paperId']\r\n",
    "        scores[paperId] = 0\r\n",
    "        for term in query_terms:\r\n",
    "            tf_in_title = len(docs[term][paperId]['title'])\r\n",
    "            tf_in_abstract = len(docs[term][paperId]['abstract'])\r\n",
    "            title_score = (1 + math.log10(tf_in_title)) * idf_in_title[term] if tf_in_title!=0 else 0\r\n",
    "            abstract_score = (1 + math.log10(tf_in_abstract)) * idf_in_abstract[term] if tf_in_abstract!=0 else 0\r\n",
    "            weight_t_d = (title_score_weight * title_score) + (1-title_score_weight)*abstract_score\r\n",
    "            scores[paperId] += weight_t_d * weight_t_q            \r\n",
    "                \r\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\r\n",
    "    if max_result_count==-1:\r\n",
    "        return sorted_scores\r\n",
    "    else:    \r\n",
    "        return (dict(list(sorted_scores.items())[:max_result_count]))\r\n",
    "        \r\n",
    "\r\n",
    "ltc_lnc_dat('oligonucleotide nonhuman',  5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'65ae2070c175657121782e39d19adb499b165e73': 5.85015134293049,\n",
       " 'd8dd489d9b9d9c7bfef33b7da8e065315c184039': 3.48508681297258,\n",
       " '740058feedddff19f1af0b7c0cc066bb6245ea04': 3.007340351281301,\n",
       " 'bea51a1116b12f2b9acaf4f50280fcb854473f9b': 2.8428109916491886,\n",
       " '47c5f68c0c508323f5cfbd90bfd6c46e3606c75d': 2.8428109916491886}"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "def okapi25(query: str, max_result_count: int, title_score_weight: float = 0.5, k=1.5, b=0.75) :\r\n",
    "    query_terms = preprocessor.preprocess(query)\r\n",
    "    number_of_documents = len(df)\r\n",
    "    scores = {}\r\n",
    "    idf_in_title = {}\r\n",
    "    idf_in_abstract = {}\r\n",
    "\r\n",
    "    for term in query_terms:\r\n",
    "        df_in_title , df_in_abstract = 0, 0\r\n",
    "        for paperId, title_abstract_dictionary in docs[term].items():\r\n",
    "            df_in_title += 1 if len(title_abstract_dictionary['title'])!=0 else 0\r\n",
    "            df_in_abstract += 1 if len(title_abstract_dictionary['abstract'])!=0 else 0\r\n",
    "        number_of_documents = len(df['paperId'])\r\n",
    "        idf_in_title[term] = math.log10(number_of_documents / df_in_title) if df_in_title!=0 else 0  \r\n",
    "        idf_in_abstract[term] = math.log10(number_of_documents / df_in_abstract) if df_in_abstract!=0 else 0 \r\n",
    "\r\n",
    "    sum_title_length = 0\r\n",
    "    sum_abstract_length = 0\r\n",
    "    for i, row in df.iterrows():\r\n",
    "        sum_title_length += len(row['title'])\r\n",
    "        sum_abstract_length += len(row['abstract'])\r\n",
    "    avg_title_length = sum_title_length / number_of_documents\r\n",
    "    avg_abstract_length = sum_abstract_length / number_of_documents\r\n",
    "\r\n",
    "    for i, row in df.iterrows():\r\n",
    "        paperId = row['paperId']\r\n",
    "        scores[paperId] = 0\r\n",
    "        for term in query_terms:\r\n",
    "            tf_in_title = len(docs[term][paperId]['title'])\r\n",
    "            tf_in_abstract = len(docs[term][paperId]['abstract'])\r\n",
    "            formula_title = ((k+1) * tf_in_title) / (k*((1-b)+b*(len(row['title']) / avg_title_length)) + tf_in_title)\r\n",
    "            formula_abstract = ((k+1) * tf_in_abstract) / (k*((1-b)+b*(len(row['abstract']) / avg_abstract_length)) + tf_in_abstract)\r\n",
    "            RSV_title = idf_in_title[term] * formula_title\r\n",
    "            RSV_abstract = idf_in_abstract[term] * formula_abstract\r\n",
    "            RSV_doc_term = title_score_weight*RSV_title  + (1-title_score_weight)*RSV_abstract \r\n",
    "            scores[paperId] += RSV_doc_term\r\n",
    "\r\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\r\n",
    "    if max_result_count==-1:\r\n",
    "        return (sorted_scores)\r\n",
    "    else:    \r\n",
    "        return (dict(list(sorted_scores.items())[:max_result_count]))       \r\n",
    "\r\n",
    "okapi25('oligonucleotide nonhuman',  4)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'65ae2070c175657121782e39d19adb499b165e73': 5.105479989551659,\n",
       " 'd8dd489d9b9d9c7bfef33b7da8e065315c184039': 3.321951163388139,\n",
       " '740058feedddff19f1af0b7c0cc066bb6245ea04': 2.6611741441445753,\n",
       " 'bea51a1116b12f2b9acaf4f50280fcb854473f9b': 2.546129831922663}"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "def search(query: str, max_result_count: int, method: str = 'ltc-lnc-tat', weight: float = 0.5, k=1.5, b=0.75):\r\n",
    "    \"\"\"\r\n",
    "    Finds relevant documents to query\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ---------------------------------------------------------------------------------------------------\r\n",
    "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\r\n",
    "                        notice that if max_result_count = -1, then you have to return all docs\r\n",
    "\r\n",
    "    method: 'ltc-lnc-tat' or 'ltc-lnc-dat' or 'okapi25'\r\n",
    "\r\n",
    "    Returns\r\n",
    "    ----------------------------------------------------------------------------------------------------\r\n",
    "    list\r\n",
    "    Retreived documents with snippet\r\n",
    "    \"\"\"\r\n",
    "    returned_docs = {}\r\n",
    "    if method=='ltc-lnc-tat':\r\n",
    "        returned_docs =  ltc_lnc_tat(query, max_result_count, weight)\r\n",
    "    elif method=='ltc-lnc-dat':\r\n",
    "        returned_docs = ltc_lnc_dat(query, max_result_count, weight)  \r\n",
    "    elif method=='okapi25':\r\n",
    "        returned_docs = okapi25(query, max_result_count,  weight, k, b)\r\n",
    "    return returned_docs\r\n",
    "\r\n",
    "search('deep neural network', 10, 'okapi25')              "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'081651b38ff7533550a3adfc1c00da333a8fe86c': 4.458947017076259,\n",
       " 'daf74c34f7da0695b154f645c8b78a7397a98f16': 4.447506062250203,\n",
       " '1a3c74c7b11ad5635570932577cdde2a3f7a6a5c': 4.312638880259593,\n",
       " '25e3cad9f9f7191e5057975c9c2d3f2fda24ff96': 4.309263983501473,\n",
       " '20f77d34ab1aec7d0a6e613a740aff7ec7fbf55a': 4.087625343279374,\n",
       " '642d0f49b7826adcf986616f4af77e736229990f': 4.077109406078264,\n",
       " '9cc8609f904c50b1be408abede7ab7f5cdbc9744': 3.8690031992993648,\n",
       " '2e2b189f668cf2c06ebc44dc9b166648256cf457': 3.855160089891143,\n",
       " '05fd1da7b2e34f86ec7f010bef068717ae964332': 3.7700160193786623,\n",
       " 'afff02fa27bdeb0e2dc658d1ce43af6d3f407ff5': 3.7616640006605744}"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1>3.\n",
    "مدل‌های زبانی\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش یک سیستم مبتنی بر مدل‌های زبانی پیاده‌سازی می‌کنید. مدل زبانی‌ای که باید استفاده کنید مدل \n",
    "unigram\n",
    "است که می‌توان گفت ساده‌ترین مدل ممکن است. در این مدل ما برای هر داک یک مدل دظر نظر می‌گیریم و احتمال حضور کلمات در مدل یک داک \n",
    "نسبت به هم مستقل هستند. در پیاده‌سازی این بخش دستتان خیلی باز است. مدل‌هایی که تشکیل می‌دهید و نحوه \n",
    "smoothing \n",
    "و همچنین پارامترهای مدل همگی به خودتان برمی‌گردد. \n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "def lm_search(query: str, max_result_count: int, lamda_value = 0.8):\r\n",
    "    \"\"\"\r\n",
    "    Finds relevant documents to query.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ---------------------------------------------------------------------------------------------------\r\n",
    "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\r\n",
    "                        notice that if max_result_count = -1, then you have to return all docs\r\n",
    "\r\n",
    "    Returns\r\n",
    "    ----------------------------------------------------------------------------------------------------\r\n",
    "    list\r\n",
    "    Retreived documents with snippet\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    query_terms = preprocessor.preprocess(query)\r\n",
    "    scores = {}\r\n",
    "\r\n",
    "    #Using Jelinek-Mercer smoothing method\r\n",
    "    total_number_of_tokens = 0\r\n",
    "    for _, row in df.iterrows():\r\n",
    "        total_number_of_tokens += len(row['title']) + len(row['abstract'])\r\n",
    "\r\n",
    "    for _, row in df.iterrows():\r\n",
    "        paperId = row['paperId']\r\n",
    "        scores[paperId] = 1\r\n",
    "        for term in query_terms:\r\n",
    "            prob_term_docmodel = get_tf(term, paperId) / (len(row['title']) + len(row['abstract']))\r\n",
    "            collection_freq_t = 0\r\n",
    "            for docId in docs[term]:\r\n",
    "                collection_freq_t += len(docs[term][docId]['title']) + len(docs[term][docId]['abstract'])\r\n",
    "            prob_term_collection = collection_freq_t / total_number_of_tokens\r\n",
    "            prob_term_doc =  lamda_value*prob_term_docmodel + (1-lamda_value)*prob_term_collection\r\n",
    "            scores[paperId] *= prob_term_doc  \r\n",
    "\r\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\r\n",
    "    if max_result_count==-1:\r\n",
    "        return (sorted_scores)\r\n",
    "    else:    \r\n",
    "        return (dict(list(sorted_scores.items())[:max_result_count]))     \r\n",
    "\r\n",
    "lm_search('oligonucleotide nonhuman', 5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'65ae2070c175657121782e39d19adb499b165e73': 0.00012705802489778006,\n",
       " '740058feedddff19f1af0b7c0cc066bb6245ea04': 5.562240313671596e-08,\n",
       " 'd8dd489d9b9d9c7bfef33b7da8e065315c184039': 4.7756348885319623e-08,\n",
       " 'dcf23b79909efe40a7d7ef8fdf84eee6b36bbc59': 4.249347638091316e-08,\n",
       " 'c8e7b7815024b5aaab6a851e6123577bf6b74344': 3.653416546541706e-08}"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1>4.\n",
    "ارزیابی عملکرد سامانه\n",
    "</h1>\n",
    "<p>\n",
    "در این بخش معیارهای زیر را برای سیستم‌های طراحی شده پیاده‌سازی کنید. سپس در بخش آخر با فراخوانی توابع پیاده‌سازی شده و ورودی دادن مقادیر موجود در فایل\n",
    "validation.json\n",
    "به عنوان ورودی \n",
    "actual\n",
    "توابع، می‌توانید معیارها را برای سیستم‌های بازیابی خودتان بدست بیاورید. \n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "class Evaluation:\r\n",
    "    def __init__(self, actual, predicts):\r\n",
    "        self.actual = actual\r\n",
    "        self.predicts = predicts\r\n",
    "\r\n",
    "    def evaluate(self, require_scores):\r\n",
    "        \"\"\"\r\n",
    "        Prints require scores for actual and predicts array\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        require_scores : List[str]\r\n",
    "            Scores required to calculated\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        \"\"\"\r\n",
    "        for score_type in require_scores:\r\n",
    "            if score_type == \"precision\":\r\n",
    "                print(f'Precision: {self.precision()}')\r\n",
    "            elif score_type == \"recall\":\r\n",
    "                print(f'Recall: {self.recall()}')\r\n",
    "            elif score_type == \"f1\":\r\n",
    "                print(f'F1 Score: {self.f1()}')\r\n",
    "            elif score_type == \"map\":\r\n",
    "                print(f'MAP Score: {self.map()}')\r\n",
    "            elif score_type == \"ndcg\":\r\n",
    "                print(f'NDCG Score: {self.ndcg()}')\r\n",
    "            elif score_type == \"mrr\":\r\n",
    "                print(f'MRR Score: {self.mrr()}')\r\n",
    "            else:\r\n",
    "                print(f'Unknown score type is given: {score_type}')\r\n",
    "\r\n",
    "    def precision(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the precision of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The precision of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        precision = sum(len(set(pred) & set(act)) / len(set(pred)) if len(set(pred)) > 0 else 0\r\n",
    "                     for act, pred in zip(self.actual, self.predicts)) / len(self.actual)\r\n",
    "\r\n",
    "        return precision\r\n",
    "\r\n",
    "    def recall(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the recall of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The recall of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        recall = sum(len(set(pred) & set(act)) / len(set(act)) if len(set(act)) > 0 else 0\r\n",
    "                     for act, pred in zip(self.actual, self.predicts)) / len(self.actual)\r\n",
    "\r\n",
    "        return recall\r\n",
    "\r\n",
    "    def f1(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the F1 score of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The F1 score of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        precision = self.precision()\r\n",
    "        recall = self.recall()\r\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\r\n",
    "        return f1_score\r\n",
    "\r\n",
    "    def map(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the Mean Average Precision of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The Mean Average Precision of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        total_avg_precision = 0\r\n",
    "        num_samples = len(actuals)\r\n",
    "\r\n",
    "        for one_query_results in range(num_samples):\r\n",
    "            actual_docs = set(actuals[one_query_results])\r\n",
    "            predicted_docs = predicts[one_query_results]\r\n",
    "            precision_at_k_sum = 0\r\n",
    "            correct_predictions = 0\r\n",
    "\r\n",
    "            for position, pred_doc in enumerate(predicted_docs, 1):\r\n",
    "                if pred_doc in actual_docs:\r\n",
    "                    correct_predictions += 1\r\n",
    "                    precision_at_k = correct_predictions / position\r\n",
    "                    precision_at_k_sum += precision_at_k\r\n",
    "            if correct_predictions > 0:\r\n",
    "                avg_precision = precision_at_k_sum / len(actual_docs)\r\n",
    "                total_avg_precision += avg_precision\r\n",
    "\r\n",
    "        mean_avg_precision = total_avg_precision / num_samples\r\n",
    "        return mean_avg_precision\r\n",
    "\r\n",
    "    def ndcg(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the Normalized Discounted Cumulative Gain (NDCG) of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The NDCG of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        ndcg_sum = 0.0\r\n",
    "        num_samples = len(actuals)\r\n",
    "\r\n",
    "        for i in range(num_samples):\r\n",
    "            actual_docs = set(actuals[i])\r\n",
    "            predicted_docs = predicts[i]\r\n",
    "            dcg, idcg = 0, 0\r\n",
    "\r\n",
    "            for j, (docId, pred_score) in enumerate(predicted_docs.items()):\r\n",
    "                relevance = 1 if docId in actual_docs else 0\r\n",
    "                dcg += (2 ** relevance-1) / (math.log2(j+2))\r\n",
    "\r\n",
    "            sorted_actual_labels = sorted(actual_docs, reverse=True)\r\n",
    "            for k in range(len(sorted_actual_labels)):\r\n",
    "                idcg += 1 / math.log2(k + 2)\r\n",
    "\r\n",
    "            if idcg > 0:\r\n",
    "                ndcg_sum += dcg / idcg\r\n",
    "\r\n",
    "        return ndcg_sum / num_samples if num_samples>0 else 0\r\n",
    "\r\n",
    "    def mrr(self):\r\n",
    "        \"\"\"\r\n",
    "        Calculates the Mean Reciprocal Rank of the predicted results\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        actual : List[List[str]]\r\n",
    "            The actual results\r\n",
    "        predicted : List[List[str]]\r\n",
    "            The predicted results\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        float\r\n",
    "            The MRR of the predicted results\r\n",
    "        \"\"\"\r\n",
    "        ranks_list = []\r\n",
    "\r\n",
    "        for i in range(len(actuals)):\r\n",
    "            actual_docs = set(actuals[i])\r\n",
    "            predicted_docs = predicts[i]\r\n",
    "\r\n",
    "            for j, pred_doc in enumerate(predicted_docs, 1):\r\n",
    "                if pred_doc in actual_docs:\r\n",
    "                    ranks_list.append(1/j)\r\n",
    "                    break\r\n",
    "\r\n",
    "        return sum(ranks_list) / len(ranks_list) if ranks_list else 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "require_scores = [\"precision\", \"recall\", \"f1\", \"map\", \"ndcg\", \"mrr\"]\r\n",
    "\r\n",
    "with open('validation.json', 'r') as json_file:\r\n",
    "    validation_data = json.load(json_file)\r\n",
    "\r\n",
    "actuals = list(validation_data.values())\r\n",
    "queries = list(validation_data.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "predicts = [search(query, 10, method='ltc-lnc-tat') for query in queries]\r\n",
    "predicts_docs = []\r\n",
    "for doc_score_dict  in predicts:\r\n",
    "    predicts_docs.append(list(doc_score_dict.keys()))\r\n",
    "\r\n",
    "eval_vsm = Evaluation(actuals, predicts_docs)\r\n",
    "eval_vsm.evaluate(require_scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.6166666666666667\n",
      "Recall: 0.6166666666666667\n",
      "F1 Score: 0.6166666666666667\n",
      "MAP Score: 0.5290476190476191\n",
      "NDCG Score: 0.6553118545505704\n",
      "MRR Score: 0.7777777777777777\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "predicts = [search(query, 10, method='okapi25') for query in queries]\r\n",
    "predicts_docs = []\r\n",
    "for doc_score_dict  in predicts:\r\n",
    "    predicts_docs.append(list(doc_score_dict.keys()))\r\n",
    "    \r\n",
    "eval_bm = Evaluation(actuals, predicts_docs)\r\n",
    "eval_bm.evaluate(require_scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.6166666666666667\n",
      "Recall: 0.6166666666666667\n",
      "F1 Score: 0.6166666666666667\n",
      "MAP Score: 0.5545304232804233\n",
      "NDCG Score: 0.684585569899856\n",
      "MRR Score: 0.9166666666666666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "predicts = [lm_search(query, 10) for query in queries]\r\n",
    "\r\n",
    "eval_lm = Evaluation(actuals, predicts)\r\n",
    "eval_lm.evaluate(require_scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.6166666666666667\n",
      "Recall: 0.6166666666666667\n",
      "F1 Score: 0.6166666666666667\n",
      "MAP Score: 0.5528505291005291\n",
      "NDCG Score: 0.6286122946838343\n",
      "MRR Score: 0.7333333333333334\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}