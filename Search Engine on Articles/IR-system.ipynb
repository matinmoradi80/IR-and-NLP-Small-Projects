{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین اول</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۱۵ آبان <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "<h1>مقدمه</h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "این تمرین به پیش‌پردازش متن، اصلاح پرسمان، ساخت نمایه، بازیابی boolean و فشرده‌سازی نمایه می‌پردازد.\n",
    "<br>\n",
    "دیتاستی که در اختبار شما قرار گرفته است شامل چکیده مقالات و id آن‌ها می‌باشد.\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1> آماده‌سازی دیتاست</h1>\n",
    "<p>\n",
    "دیتاستی که در اختیار شما قرار گرفته است، دارای سطر‌هایی می‌باشد که دارای مقدار NaN می‌باشد. برای اینکه بتوانید با این دیتاست کار کنید، باید ابتدا این سطر‌ها را حذف کنید.\n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "file_path = 'data.csv'\r\n",
    "df = pd.read_csv(file_path)\r\n",
    "print('\\nBEFORE:')\r\n",
    "print(df[80:90])\r\n",
    "df = df.dropna()\r\n",
    "df = df.reset_index(drop=True)\r\n",
    "print('\\nAFTER:')\r\n",
    "print(df[80:90])\r\n",
    "articles_dict = df.set_index('paperId')['abstract'].to_dict()\r\n",
    "print('\\narticles_dict[81]: ' + articles_dict[81])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "BEFORE:\n",
      "    paperId                                           abstract\n",
      "80       81  We present a convex optimization based Symbol-...\n",
      "81       82                                                NaN\n",
      "82       83  Image denoising is still a challenging issue i...\n",
      "83       84  \"The emergence of video streaming applications...\n",
      "84       85  Since the advent of high-performance distribut...\n",
      "85       86  Wearable devices are an unobtrusive, cost-effe...\n",
      "86       87  The traffic volume that needs to be handled by...\n",
      "87       88  In this paper we present an approach to multi-...\n",
      "88       89  The book is a research monograph that incorpor...\n",
      "89       90                                                NaN\n",
      "\n",
      "AFTER:\n",
      "    paperId                                           abstract\n",
      "80       81  We present a convex optimization based Symbol-...\n",
      "81       83  Image denoising is still a challenging issue i...\n",
      "82       84  \"The emergence of video streaming applications...\n",
      "83       85  Since the advent of high-performance distribut...\n",
      "84       86  Wearable devices are an unobtrusive, cost-effe...\n",
      "85       87  The traffic volume that needs to be handled by...\n",
      "86       88  In this paper we present an approach to multi-...\n",
      "87       89  The book is a research monograph that incorpor...\n",
      "88       91  This paper presents a system-level scheme to a...\n",
      "89       92  Using three-dimensional technology computer-ai...\n",
      "\n",
      "articles_dict[81]: We present a convex optimization based Symbol-Level Precoding (SLP) for sum power minimization and propose the low-latency closed-form algorithm to find a heuristic solution to the optimization problem. The technique exploits constructive interference at the multi-user MIMO systems and minimizes the sum power of the transmitted precoded signal per each set of MIMO symbols. As a result, the received signals gain extra Signal-to-Noise Ratio (SNR), which improves data rate and energy efficiency of the system. We benchmark the low-complexity algorithm for solving the optimization technique against the conventional Fast Non-Negative Least Squares algorithm (NNLS). The demonstrated design of the SLP technique combined with the proposed closed-form algorithm has low computational complexity and fast processing time, which is applicable in low-latency high-throughput satellite communication systems.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h1> Preprocessing (پیش پردازش)</h1>\r\n",
    "<p>\r\n",
    "بسیاری از داده ‌ها دارای مقادیر زیادی اطلاعات اضافه هستند که در پردازش ها به آن نیازی نیست و یا باعث ایجاد خطا میشوند.\r\n",
    "دراین بخش داده را از دیتابیس مورد نظر خوانده\r\n",
    "  و سپس پیش پردازش های مورد نیاز را اعمال کنید تا متن پیش پردازش شده را تولید کنید.\r\n",
    "پس از اتمام پیش پردازش سایر عملیات گفته شده در ادامه را بر\r\n",
    "روی متن ایجاد شده انجام میدهیم.\r\n",
    "\r\n",
    "\r\n",
    "کلاس\r\n",
    "\"Preprocessor\"\r\n",
    "عملیات پیش پردازش را انجام میدهد. نام توابع عمل های مورد نظر نوشته شده است و که با توجه به آن باید کد مخصوص هر یک نوشته شود. تابع\r\n",
    "\"preprocessor\"\r\n",
    "تابع اصلی این کلاس است که متن بدون پیش پردازش را گرفته و پردازش های مورد نظر را در آن اعمال میکند و متن مورد نظر را ایجاد میکند.\r\n",
    "\r\n",
    "در این بخش میتوانید از کتابخانه های آماده مانند\r\n",
    "<a href=\"https://www.nltk.org/\">NLTK</a>\r\n",
    "و\r\n",
    "<a href=\"https://spacy.io/\">SpaCy</a>\r\n",
    "استفاده کنید.\r\n",
    "</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import string\r\n",
    "\r\n",
    "nltk.download('stopwords')\r\n",
    "print('Stopwords:')\r\n",
    "print(stopwords.words('english'))\r\n",
    "print('\\nPunctuations:')\r\n",
    "print(string.punctuation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuations:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "source": [
    "!pip install spacy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py\", line 696, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py\", line 964, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connection.py\", line 364, in connect\n",
      "    conn = self._connect_tls_proxy(hostname, conn)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connection.py\", line 501, in _connect_tls_proxy\n",
      "    socket = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\util\\ssl_.py\", line 453, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\util\\ssl_.py\", line 495, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 500, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1040, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1309, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1129)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\util\\retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\cli\\_util.py\", line 87, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\cli\\download.py\", line 36, in download_cli\n",
      "    download(model, direct, sdist, *ctx.args)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\cli\\download.py\", line 70, in download\n",
      "    compatibility = get_compatibility()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\cli\\download.py\", line 94, in get_compatibility\n",
      "    r = requests.get(about.__compatibility__)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\adapters.py\", line 514, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "import string\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "\r\n",
    "class Preprocessor:\r\n",
    "\r\n",
    "    def __init__(self, given_stopwords):\r\n",
    "        self.stopwords = given_stopwords\r\n",
    "\r\n",
    "    def preprocess(self, text):\r\n",
    "        text = self.remove_links_and_tags(text)\r\n",
    "        words = self.word_tokenize(text)\r\n",
    "        stopwords_removed_words = self.remove_stopwords(words)\r\n",
    "        punctuation_removed_words = self.remove_punctuations(stopwords_removed_words)\r\n",
    "        final_words = self.normalize(punctuation_removed_words)\r\n",
    "        return final_words      \r\n",
    "\r\n",
    "    def remove_links_and_tags(self, text):\r\n",
    "        link_removed_text = re.sub(r'http\\S+', '', text)\r\n",
    "        clean_text = re.sub(r'<.*?>', '', link_removed_text)\r\n",
    "        return clean_text\r\n",
    "\r\n",
    "    def word_tokenize(self, text):\r\n",
    "        words = word_tokenize(text)\r\n",
    "        return words\r\n",
    "\r\n",
    "    def remove_stopwords(self, words):\r\n",
    "        stopwords_collection = self.stopwords + stopwords.words('english')\r\n",
    "        words = [word for word in words if word not in stopwords_collection]\r\n",
    "        return words\r\n",
    "\r\n",
    "    def remove_punctuations(self, words):\r\n",
    "        punctuaction_removed_words = [''.join([char for char in word if char not in string.punctuation]) for word in words]\r\n",
    "        return punctuaction_removed_words\r\n",
    "\r\n",
    "    def normalize(self, words):\r\n",
    "        doc = nlp(' '.join(words))\r\n",
    "        normalized_words = [token.lemma_.lower() for token in doc if len(token.text) > 2 and not token.is_space]\r\n",
    "        return normalized_words\r\n",
    "\r\n",
    "        # lemmatizer = WordNetLemmatizer()\r\n",
    "        # normalized_words=[]\r\n",
    "        # for word in words:\r\n",
    "        #     if len(word) > 2:\r\n",
    "        #         normalized_words.append(lemmatizer.lemmatize(word.lower()))\r\n",
    "        # return normalized_words\r\n",
    "        # NOTE: NLTK Lemmatizer was not performing very good. So I used Spacy instead which is stronger\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "source": [
    "text = \"\"\"An NoC (network on chip) constructed with silicon photonic microrings can be implemented with CMOS technology and integrated with processor cores on the same die. Each microring in the NoC has two switching states, one consuming significantly less power than the other. Different routing schemes can lead to different numbers of microrings in the high-power consumption state, and result in different power consumptions for the NoC. In an earlier work, a looping-based routing algorithm was proposed which exploits the different power consumption characteristics of the two switching states of a microring so as to minimize the power consumption of a Benes-type NoC; but this algorithm cannot provide an optical solution. In this paper, we will show the necessary and sufficient conditions for finding the optimal solution for a Benes-type NoC. We also present a new routing algorithm. Compared with the algorithm of the previous work, the new algorithm can reduce the number of microrings in the high-power-consumption state by as many as <inline-formula> <tex-math notation=\"LaTeX\">$(n - 2)\\cdot 2^{n-2}$ </tex-math></inline-formula> in a <inline-formula> <tex-math notation=\"LaTeX\">$2^{n}\\times 2^{n}$ </tex-math></inline-formula> Benes-type NoC for certain interconnection patterns.\"\"\"\r\n",
    "preprocessor = Preprocessor([])\r\n",
    "cleaned_words = preprocessor.preprocess(text)\r\n",
    "print(cleaned_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['noc', 'network', 'chip', 'construct', 'silicon', 'photonic', 'microring', 'implement', 'cmos', 'technology', 'integrate', 'processor', 'core', 'die', 'each', 'microre', 'noc', 'two', 'switch', 'state', 'one', 'consume', 'significantly', 'less', 'power', 'different', 'routing', 'scheme', 'lead', 'different', 'number', 'microring', 'highpower', 'consumption', 'state', 'result', 'different', 'power', 'consumption', 'noc', 'early', 'work', 'loopingbase', 'routing', 'algorithm', 'propose', 'exploit', 'different', 'power', 'consumption', 'characteristic', 'two', 'switch', 'state', 'microre', 'minimize', 'power', 'consumption', 'benestype', 'noc', 'algorithm', 'provide', 'optical', 'solution', 'paper', 'show', 'necessary', 'sufficient', 'condition', 'find', 'optimal', 'solution', 'benestype', 'noc', 'also', 'present', 'new', 'routing', 'algorithm', 'compare', 'algorithm', 'previous', 'work', 'new', 'algorithm', 'reduce', 'number', 'microring', 'highpowerconsumption', 'state', 'many', 'cdot', 'ime', 'benestype', 'noc', 'certain', 'interconnection', 'pattern']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1>ساخت نمایه</h1>\n",
    "<p>\n",
    "شما در حال توسعه یک موتور جستجوی سریع هستید که از نمایه سازی پویا پشتیبانی می کند. موتور جستجو اسناد جدید را در قالب دسته‌هایی کوچک‌تر   \n",
    "(batch) هندل می‌کند. در پایان هر روز، این دسته‌ها با استفاده از استراتژی ادغام لگاریتمی ادغام می شوند. هدف به حداقل رساندن هزینه ادغام است.  \n",
    "مراحلی که باید برای حل این مسئله انجام دهید عبارتند از:\n",
    "<li>توکن‌بندی و نرمال‌سازی متن از اسناد.</li>\n",
    "    <li>ایجاد یک index مرتب‌ شده برای هر دسته از اسناد.</li>\n",
    "    <li>ادغام بهینه چند دسته از indexها با استفاده از یک استراتژی ادغام لگاریتمی.</li>\n",
    "وظیفه شما این است که بخش‌های خالی <strong>(مشخص شده به‌صورت {TODO})</strong> کد را پر کنید تا موتور جستجو عملی شود.\n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h3> دستورات </h3>\r\n",
    "<li>متد <code>sort_based_index_construction</code> از <code>DocumentBatch</code>:</li>\r\n",
    "<p>هر سند را با استفاده از تابع‌هایی که در قسمت قبل نوشتید، عمل preprocessing را روی آن انجام دهید.</p>\r\n",
    "<p>برای هر توکن، شناسه سند را به فهرست معکوس (inverted index) برای آن توکن اضافه کنید.</p>\r\n",
    "<li>متد <code>add_batch</code> در <code>FastSearchEngine</code></li>\r\n",
    "<p>فهرست معکوس برای دسته (batch) را ایجاد کنید.</p>\r\n",
    "<p>این دسته را به فهرست‌های روزانه اضافه کنید.</p>\r\n",
    "<li>متد <code>end_of_day_merge</code> از <code>FastSearchEngine:</code></li>\r\n",
    "<p>استراتژی ادغام لگاریتمی را پیاده‌سازی کنید تا به صورت بهینه فهرست‌های روزانه را با فهرست اصلی ادغام کنید.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "source": [
    "#ّFor tessting the merge algorithm\r\n",
    "\r\n",
    "from collections import deque\r\n",
    "\r\n",
    "def merge_two_inverted_indexes(index1, index2):\r\n",
    "    merged_index = {}\r\n",
    "    for term, postings1 in index1.items():\r\n",
    "        if term in index2:\r\n",
    "            postings2 = index2[term]\r\n",
    "            merged_index[term] = postings1.union(postings2)\r\n",
    "            del index2[term]\r\n",
    "        else:\r\n",
    "            merged_index[term] = postings1\r\n",
    "    merged_index.update(index2)\r\n",
    "    return merged_index\r\n",
    "\r\n",
    "index1 = {\"term1\": {1, 2, 3}, \"term2\": {4, 5, 6}}\r\n",
    "index2 = {\"term2\": {7, 8, 9}, \"term3\": {10, 11, 12}}\r\n",
    "index3 = {\"term2\": {1, 2}}\r\n",
    "index4 = {\"term1\": {7, 8, 9, 10}, \"term3\": {1, 3, 2, 4, 8, 9}}\r\n",
    "\r\n",
    "all_indexes = deque([index1, index2, index3, index4])\r\n",
    "new_indexes = sorted(all_indexes, key=lambda index: sum(len(doc_ids) for doc_ids in index.values()))\r\n",
    "all_indexes = deque(new_indexes)\r\n",
    "print(f\"Before merge After sort:\\n {all_indexes}\")\r\n",
    "\r\n",
    "while len(all_indexes)>1:\r\n",
    "    first_index = all_indexes.popleft()\r\n",
    "    second_index = all_indexes.popleft()\r\n",
    "    merged_two_indexes = merge_two_inverted_indexes(first_index, second_index)\r\n",
    "    all_indexes.append(merged_two_indexes)\r\n",
    "\r\n",
    "print(f\"After merge:\\n {all_indexes}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before merge After sort:\n",
      " deque([{'term2': {1, 2}}, {'term1': {1, 2, 3}, 'term2': {4, 5, 6}}, {'term2': {8, 9, 7}, 'term3': {10, 11, 12}}, {'term1': {8, 9, 10, 7}, 'term3': {1, 2, 3, 4, 8, 9}}])\n",
      "After merge:\n",
      " deque([{'term2': {1, 2, 4, 5, 6, 7, 8, 9}, 'term1': {1, 2, 3, 7, 8, 9, 10}, 'term3': {1, 2, 3, 4, 8, 9, 10, 11, 12}}])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "source": [
    "from collections import defaultdict, deque\r\n",
    "\r\n",
    "def merge_two_inverted_indexes(index1, index2):\r\n",
    "    merged_index = {}\r\n",
    "    for term, postings1 in index1.items():\r\n",
    "        if term in index2:\r\n",
    "            postings2 = index2[term]\r\n",
    "            merged_index[term] = postings1.union(postings2)\r\n",
    "            del index2[term]\r\n",
    "        else:\r\n",
    "            merged_index[term] = postings1\r\n",
    "    merged_index.update(index2)\r\n",
    "    return merged_index\r\n",
    "\r\n",
    "class DocumentBatch:\r\n",
    "    def __init__(self, docs):\r\n",
    "        self.documents = docs\r\n",
    "        self.index = defaultdict(set)\r\n",
    "        self.preprocessor = Preprocessor([])\r\n",
    "\r\n",
    "    def sort_based_index_construction(self):\r\n",
    "        for doc_id, doc in self.documents.items():\r\n",
    "            terms = preprocessor.preprocess(doc)\r\n",
    "            for term in terms:\r\n",
    "                self.index[term].add(doc_id)\r\n",
    "\r\n",
    "class FastSearchEngine:\r\n",
    "    def __init__(self):\r\n",
    "        self.main_index = defaultdict(set)\r\n",
    "        self.daily_indices = deque()\r\n",
    "\r\n",
    "    def add_batch(self, batch: DocumentBatch):\r\n",
    "        batch.sort_based_index_construction()\r\n",
    "        batch_inverted_index = batch.index\r\n",
    "        self.daily_indices.append(batch_inverted_index)\r\n",
    "        \r\n",
    "\r\n",
    "    def end_of_day_logarithmic_merge(self):        \r\n",
    "        all_indexes = self.daily_indices\r\n",
    "        new_indexes = sorted(all_indexes, key=lambda index: sum(len(doc_ids) for doc_ids in index.values()))\r\n",
    "        all_indexes = deque(new_indexes)\r\n",
    "        while len(all_indexes)>1:\r\n",
    "            first_index = all_indexes.popleft()\r\n",
    "            second_index = all_indexes.popleft()\r\n",
    "            merged_two_indexes = merge_two_inverted_indexes(first_index, second_index)\r\n",
    "            all_indexes.append(merged_two_indexes)    \r\n",
    "        self.main_index = merge_two_inverted_indexes(all_indexes.pop(), self.main_index)   \r\n",
    "        self.daily_indices.clear "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "source": [
    "# Divide the documents into groups to distribute them between servers. For example, let's consider two servers here.\r\n",
    "# Divide the documents of each server into batches, for instance, five batches.\r\n",
    "# Create an index for each batch and for each server, then merge them into the main index at the end of the day.\r\n",
    "# Repeat this process until all documents are processed.\r\n",
    "\r\n",
    "split_position = 3081\r\n",
    "df1 = df.iloc[:split_position]\r\n",
    "df2 = df.iloc[split_position:]\r\n",
    "\r\n",
    "search_engine = FastSearchEngine()\r\n",
    "for i in range(13):\r\n",
    "    docs1 = df1[i*237:i*237+237].set_index('paperId')['abstract'].to_dict()\r\n",
    "    docs2 = df2[i*237:i*237+237].set_index('paperId')['abstract'].to_dict()\r\n",
    "    server_a_batch = DocumentBatch(docs1)\r\n",
    "    server_b_batch = DocumentBatch(docs2)\r\n",
    "\r\n",
    "    search_engine.add_batch(server_a_batch)\r\n",
    "    search_engine.add_batch(server_b_batch)\r\n",
    "    print(f'processing i number: {i} with main_index size= {len(search_engine.main_index)}')\r\n",
    "    search_engine.end_of_day_logarithmic_merge()\r\n",
    "\r\n",
    "final_index = search_engine.main_index\r\n",
    "print(f\"Size of the Dictionary: {len(final_index)}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing i number: 0 with main_index size= 0\n",
      "processing i number: 1 with main_index size= 6875\n",
      "processing i number: 2 with main_index size= 10433\n",
      "processing i number: 3 with main_index size= 13651\n",
      "processing i number: 4 with main_index size= 16228\n",
      "processing i number: 5 with main_index size= 18792\n",
      "processing i number: 6 with main_index size= 20918\n",
      "processing i number: 7 with main_index size= 22853\n",
      "processing i number: 8 with main_index size= 24785\n",
      "processing i number: 9 with main_index size= 26593\n",
      "processing i number: 10 with main_index size= 28489\n",
      "processing i number: 11 with main_index size= 30561\n",
      "processing i number: 12 with main_index size= 32096\n",
      "Size of the Dictionary: 33781\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "source": [
    "final_index['laptop']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1288, 1500, 2052, 2110, 2545, 2568, 3221, 5009, 5250, 5875, 6106}"
      ]
     },
     "metadata": {},
     "execution_count": 452
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h1>Spell Correction (اصلاح پرسمان)</h1>\r\n",
    "<p>\r\n",
    "در بسیاری از اوقات پرسمان دادە شده توسط کاربر، ممکن است ناقص یا دارای غلط املایی باشد. برای رفع این مشکل در بسیاری از موتورهای جستجو راە حل هایی تدارک دیده شدە است. ابتدا این راە حل ها را شرح دهید و بیان کنید که یک موتور جست و جو بر چه اساسی پرسمان های اصلاح شده را به کاربر نمایش می دهد.</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\r\n",
    "<h2>پاسخ سوال بالا</h2>\r\n",
    "برای اصلاح پرسمان، به طور کلی دو راهکار وجود دارد. یا اصلاح به صورت ایزوله اننجام میشود یا با توجه به سیاق. درحالت اول فقط خود کلمه بررسی میشود و اگر مثلا اشکال املایی در نوشتنش باشد، اصلاح خواهد شد. حالت دوم ولی به کلمات اطراف کلمه مورد نظر هم نگاه میکند فلذا اگر حتی کلمه از لحاظ املایی مشکل نداشته باشد ولی به لحاظ معنا جوری باشد که احتمالا اشتباهی رخ داده، این را هم اصلاح میکند.\r\n",
    "\r\n",
    "برای دسته اول دو راه مطرح میشود. یکی بررسی فاصله با روش Levenshtein مثلا. و دیگری هم استفاده از k-gram.\r\n",
    "در روش edit distance ما یک لیست از کلمات درست داریم و برای اصلاح یک ترم، فاصله آن ترم با کلمات درست محاسبه میشود. کلماتی که این فاصله برایشان کمترین هست، میتوانند گزینه هایی برای اصلاح ترم مورد نظر باشند. برای محاسبه فاصله میتوان از فاصله Levenshtein استفاده کرد.\r\n",
    "در روش k-gram ترکیبات دوتایی یا سه تایی یا ... حروف درکنار هم بررسی شده و یک index تشکیل میدهیم که کلید آن این k-gramها و value آن کلمات درستی هستند که آن K-GRAM درشان وجود دارد. کلمه پیشنهادی برای یک ترم، کلمه ای است که k-gram \r\n",
    "های آن ترم در کلمه پیشنهادی بین کلمات درست بیشتر یافت شود.\r\n",
    "\r\n",
    "در روش حساس به context، میتوان از روش hit-based استفاده کرد. به این معنا که برای هر ترم در کوئری، کلمات پیشنهادی جایگزین را امتحان میکنیم و می بینیم به ازای کدام یک، نتایج بیشتری برگردانده میشود. این روش چندان بهینه نیست البته"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "    در این بخش، ابتدا با استفاده از روش bigram لغات نزدیک به لغات اصلی را پیدا کنید و در آخر با معیار minimum edit distance لغتی جایگزین را برای لغت مورد نظر پیدا کنید.  سپس برای هر پرسمان ورودی کاربر، در صورت اشتباه بودن آن، آن را تصحیح کنید. برای stopword‌ها نیز می‌توانید از لیست موجود که در قسمت‌های قبل ساختید استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "source": [
    "from typing import List, Dict\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "def create_bigram_index():\r\n",
    "    # TODO: Create the bigram index here\r\n",
    "    bigram_index = {}\r\n",
    "    \r\n",
    "    all_docs = all = stopwords.words('english') + df[\"abstract\"].tolist()\r\n",
    "    for documentation in all_docs:\r\n",
    "        preprocessor = Preprocessor([])\r\n",
    "        words = preprocessor.preprocess(documentation)\r\n",
    "        modified_words = [f\"${word}$\" for word in words]\r\n",
    "\r\n",
    "        for word in modified_words:\r\n",
    "            for i in range(len(word) - 1):\r\n",
    "                bigram = word[i:i + 2]\r\n",
    "                if bigram not in bigram_index:\r\n",
    "                    bigram_index[bigram] = set()\r\n",
    "                bigram_index[bigram].add(word)\r\n",
    "\r\n",
    "    return bigram_index\r\n",
    "\r\n",
    "bigram_index = create_bigram_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "source": [
    "print(bigram_index['ug'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'$roughness$', '$breakthrough$', '$virtualaugmented$', '$enoughflexibility$', '$hugepageaware$', '$thought$', '$throughputimprove$', '$thoughtful$', '$gpugpu$', '$thought1$', '$ugw$', '$gauge$', '$portugal$', '$throughputoriented$', '$throughsivia$', '$writethrough$', '$generalenough$', '$though$', '$debugging$', '$plugsmart$', '$playthrough$', '$mlaugmented$', '$portuguese$', '$cutthrough$', '$walkthrough$', '$throughputimprovement$', '$cpugpufpga$', '$plug$', '$cpugpu$', '$thoroughly$', '$plugout$', '$hugely$', '$acceleratoraugmented$', '$gpgpugraphics$', '$bugrevealing$', '$debuggability$', '$throughput$', '$although$', '$planungswerkzeuge$', '$feedthrough$', '$ought$', '$suggesting$', '$addressingcpugpu$', '$drug$', '$plugin$', '$throughputsensitive$', '$highestthroughput$', '$wirelessaugmente$', '$enough$', '$throughputoriente$', '$werkzeuge$', '$thorough$', '$3dplug$', '$debug$', '$ugate$', '$inaugural$', '$drugresistant$', '$payasyougo$', '$august$', '$buggy$', '$throughwater$', '$deluge$', '$trough$', '$smaug$', '$suggestion$', '$augment$', '$latencythroughput$', '$conjugate$', '$bugfinding$', '$bug$', '$rough$', '$bugredux$', '$pluggable$', '$ugc$', '$throughout$', '$bugfree$', '$lowthroughput$', '$roughly$', '$throughputbasedutility$', '$linkthroughput$', '$hauptaugenmerk$', '$throughputnormalize$', '$pugh$', '$perugia$', '$hough$', '$verfugbarkeit$', '$zugleich$', '$through$', '$throughwafer$', '$throughputdelay$', '$debuggable$', '$throughvia$', '$bugreveale$', '$bugs$', '$augmentedvirtual$', '$fugaku$', '$dataaugmentation$', '$throughsiliconvias$', '$powerthroughput$', '$throughsiliconvia$', '$eugene$', '$augmented$', '$energyfrugal$', '$straightthrough$', '$storethrough$', '$loughry$', '$bugdriven$', '$fallthrough$', '$highthroughput$', '$debuggerbase$', '$mcnaughtonyamada$', '$thoroughness$', '$debugability$', '$huge$', '$ugrizy$', '$suggest$', '$nplug$', '$workmugge$', '$struggle$', '$cpugpubased$', '$concurrencybug$', '$workmugging$', '$throughsilicon$', '$straight‐through$', '$ugcaugga$', '$daughter$', '$fraught$', '$douglas$', '$hierarchythroughout$', '$sourdough$', '$pluglevel$', '$struggling$', '$rugby$', '$designfordebug$', '$ruggedize$', '$plugandplay$', '$throughputtopower$', '$augustus$', '$augmentation$', '$delaythroughput$', '$debugger$', '$hughes$', '$taug$', '$tough$', '$cpbugs$', '$mlaugmente$'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "source": [
    "from collections import defaultdict, Counter\r\n",
    "from typing import Dict, List\r\n",
    "\r\n",
    "def edit_distance(string1, string2):\r\n",
    "    if len(string1) < len(string2):\r\n",
    "        return edit_distance(string2, string1)\r\n",
    "\r\n",
    "    if len(string2) == 0:\r\n",
    "        return len(string1)\r\n",
    "\r\n",
    "    previous_row = range(len(string2) + 1)\r\n",
    "    for i, c1 in enumerate(string1):\r\n",
    "        current_row = [i + 1]\r\n",
    "        for j, c2 in enumerate(string2):\r\n",
    "            insertions = previous_row[j + 1] + 1\r\n",
    "            deletions = current_row[j] + 1\r\n",
    "            substitutions = previous_row[j] + (c1 != c2)\r\n",
    "            current_row.append(min(insertions, deletions, substitutions))\r\n",
    "        previous_row = current_row\r\n",
    "\r\n",
    "    return previous_row[-1]\r\n",
    "\r\n",
    "\r\n",
    "def spell_correction(query, bigram_index):\r\n",
    "    corrected_query = []\r\n",
    "\r\n",
    "    query_terms = query.split()\r\n",
    "    for query_term in query_terms:\r\n",
    "        query_term = f\"${query_term.lower()}$\"\r\n",
    "        suggestions = []\r\n",
    "        for i in range(len(query_term) - 1):\r\n",
    "            bigram_in_query = query_term[i:i + 2]\r\n",
    "            if bigram_in_query in bigram_index:\r\n",
    "                suggestions.extend(bigram_index[bigram_in_query])\r\n",
    "\r\n",
    "        suggestions_counter = Counter(suggestions)\r\n",
    "        most_common_term_count = suggestions_counter.most_common(20)\r\n",
    "        top_near_words = [word for word, count in most_common_term_count]\r\n",
    "        min_distance_word = min(top_near_words, key=lambda top_near_word: edit_distance(query_term, top_near_word))\r\n",
    "        corrected_query.append(min_distance_word[1:-1])\r\n",
    "\r\n",
    "    return \" \".join(corrected_query)\r\n",
    "\r\n",
    "\r\n",
    "user_query = \"Wht is the most populr progarmming lanuage\"\r\n",
    "corrected_query = spell_correction(user_query, bigram_index)\r\n",
    "print(f\"Original Query: {user_query}\")\r\n",
    "print(f\"Corrected Query: {corrected_query}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Query: Wht is the most populr progarmming lanuage\n",
      "Corrected Query: what is the most popular programming language\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "<h1> Boolean Retrieval </h1>\n",
    "<p>\n",
    " در این قسمت هدف طراحی یک سامانەی بازیابی اطلاعات boolean می‌باشد. \n",
    "\n",
    "برای این کار ابتدا پیش پردازش‌های مورد نیاز را مانند بخش قبل بر روی متون انجام دهید و در مرحله بعد ماتریس doc−term را ایجاد کنید. در نهایت کلاس BooleanRetrievalModel را تکمیل کنید که شامل توابع preprocess_query و find_siⅿiⅼar_docs است که توضیحات هرکدام در قسمت کد موجود است. هدف نهایی این است که هرگاه کوئری به تابع find_siⅿiⅼar_docs از کلاس BooleanRetrievalModel داده شود، شناسه k تا از داک‌هایی که شامل کوئری داده شده هستند برگردانده شوند.\n",
    "</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "source": [
    "# preprocess and tokenize all documents\r\n",
    "def get_all_tokens(all_docs):\r\n",
    "    all_tokens = set()\r\n",
    "    for doc in all_docs:\r\n",
    "        cleaned_words = preprocessor.preprocess(doc)\r\n",
    "        for word in cleaned_words:\r\n",
    "            all_tokens.add(word)\r\n",
    "    return all_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "source": [
    "def make_doc_term_matrix(documents_dicts):\r\n",
    "    doc_term_matrix = {}\r\n",
    "    vocabulary = get_all_tokens(documents_dicts.values())\r\n",
    "\r\n",
    "    for paper_id, abstract in documents_dicts.items():\r\n",
    "        doc_terms = preprocessor.preprocess(abstract)\r\n",
    "        for term in vocabulary:\r\n",
    "            if term not in doc_term_matrix:\r\n",
    "                doc_term_matrix[term] = {}\r\n",
    "            doc_term_matrix[term][paper_id] = 1 if term in doc_terms else 0\r\n",
    "\r\n",
    "    return doc_term_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "source": [
    "class BooleanRetrievalModel:\r\n",
    "    def __init__(self, doc_term_matrix):\r\n",
    "        self.doc_term_matrix = doc_term_matrix\r\n",
    "\r\n",
    "    def preprocess_query(self, query):\r\n",
    "        processed_query = preprocessor.preprocess(query)\r\n",
    "        return processed_query\r\n",
    "    \r\n",
    "    def find_siⅿiⅼar_docs(self, query, k=20):\r\n",
    "        processed_query = self.preprocess_query(' '.join(query.split('AND')))\r\n",
    "        similiar_docs = {key: 1 for key in self.doc_term_matrix[processed_query[0]]}\r\n",
    "        for query_term in processed_query:\r\n",
    "            similiar_docs = {key: similiar_docs[key] & self.doc_term_matrix[query_term][key] for key in similiar_docs}\r\n",
    "        similiar_docs = [key for key in similiar_docs if similiar_docs[key]==1]    \r\n",
    "        return similiar_docs[0:k] if len(similiar_docs)>k else similiar_docs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "source": [
    "dt_matrix = make_doc_term_matrix(articles_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "در این قسمت ۳ کوئری مختلف به دلخواه خود بزنید و لیست داکیومنت‌های مرتبط با آن‌ها را برگردانید. برای کوتاه‌تر شدن لیست جواب در هر کوئری می‌توانید از عملگر‌های منطقی مانند AND استفاده کنید.\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "source": [
    "booleanIR = BooleanRetrievalModel(dt_matrix)\r\n",
    "q1 = 'modern AND model'\r\n",
    "q2 = 'network AND improvement AND software'\r\n",
    "q3 = 'server AND energy AND efficiency AND low' \r\n",
    "all_q = [q1, q2, q3]\r\n",
    "for query in all_q:\r\n",
    "    print(f'{query}:  {booleanIR.find_similar_docs(query)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "modern AND model:  [22, 23, 51, 137, 156, 389, 436, 456, 586, 616, 632, 651, 687, 709, 810, 823, 869, 911, 941, 969]\n",
      "network AND improvement AND software:  [121, 1116, 1285, 2754, 4266, 5595]\n",
      "server AND energy AND efficiency AND low:  [799, 950, 1885, 4500, 5000]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "# ذخیره و فشرده‌سازی نمایه\n",
    "در این بخش، در ابتدا دو الگوریتم فشرده‌سازی gamma code و variable byte را پیاده‌سازی کنید.  \n",
    "سپس نمایه را به سه شکل زیر ذخیره کنید:\n",
    "- نمایه‌ی اصلی بدون فشرده‌سازی\n",
    "- نمایه‌ای که با استفاده از gamma code فشرده شده است.\n",
    "- نمایه‌ای که با استفاده از variable byte فشرده شده است.\n",
    "\n",
    "در ادامه اندازه‌ی هر کدام از سه فایل بالا را با استفاده از یک تابع به دست آورده و چاپ کنید.  \n",
    "همچنین باید تابع‌هایی برای decompress کردن نمایه‌های فشرده‌شده پیاده‌سازی کنید.\n",
    "\n",
    "**نکته‌ی ۱:** تمامی نمایه‌ها را نیز در کوئرا ارسال کنید. اگر حجم‌شان بیش‌تر از محدودیت کوئرا است، آن‌ها را در یک مکان دیگر آپلود کرده و لینک آن را در این فایل قرار دهید.  \n",
    "**نکته‌ی ۲:** توابع زیر صرفاً پیشنهادی هستند و هر گونه تغییر تا زمانی که کاربردهای مورد نظر پیاده شود، آزاد است.\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "source": [
    "def gamma_encode(n):\r\n",
    "    if n == 0:\r\n",
    "        return '0'\r\n",
    "    offset = bin(n)[3:]\r\n",
    "    unary_code = '1'* len(offset) + '0'\r\n",
    "    return unary_code + offset\r\n",
    "\r\n",
    "def encode_inverted_index(inverted_index):\r\n",
    "    encoded_index = {}\r\n",
    "    for term, postings in inverted_index.items():\r\n",
    "        encoded_index[term] = ''.join([gamma_encode(doc_id) for doc_id in postings])\r\n",
    "    return encoded_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "source": [
    "def gamma_decode(n):\r\n",
    "    original_postings = set()\r\n",
    "    while len(n) > 0:\r\n",
    "        binary_lenght = 0\r\n",
    "        for i in n:\r\n",
    "            if i== '1':\r\n",
    "                binary_lenght += 1\r\n",
    "            else:\r\n",
    "                break\r\n",
    "        n = n[binary_lenght+1:]    \r\n",
    "        binary_num = '1' + n[0:binary_lenght]\r\n",
    "        if len(n) <= binary_lenght:\r\n",
    "            n=''\r\n",
    "        else:\r\n",
    "            n = n[binary_lenght:]    \r\n",
    "        decimal_num = int(binary_num, 2)\r\n",
    "        original_postings.add(decimal_num)\r\n",
    "    return original_postings \r\n",
    "\r\n",
    "def decode_inverted_index(encoded_index):\r\n",
    "    decoded_index = {}\r\n",
    "    for term, postings in encoded_index.items():\r\n",
    "        decoded_index[term] = gamma_decode(postings)\r\n",
    "    return decoded_index       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "source": [
    "print(f\"\"\"original postings:\\n{final_index['instrument']}\"\"\")\r\n",
    "gamma_ecoded_postings = encode_inverted_index(final_index)\r\n",
    "print(f\"\\nGamma encoded postings of 'instrument': {gamma_ecoded_postings['instrument']}\")\r\n",
    "gamma_decoded_postings = decode_inverted_index(gamma_ecoded_postings)\r\n",
    "print(f\"Gamma decoded postings of 'instrument':\\n{gamma_decoded_postings['instrument']}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original postings:\n",
      "{4226, 3079, 1679, 4367, 1811, 3349, 2456, 4891, 5920, 2086, 2477, 5168, 1841, 4657, 2486, 2234, 3005, 5438, 5573, 1102, 3922, 603, 2524, 1265, 3071}\n",
      "\n",
      "Gamma encoded postings of 'instrument': 1111111111110000010000010111111111110100000001111111111111010100011111111111111110000100001111111111111101100010011111111111110101000101011111111111100011001100011111111111100011000110111111111111110011100100000111111111110000001001101111111111100011010110111111111111100100001100001111111111011001100011111111111110001000110001111111111110001101101101111111111100001011101011111111111001110111101111111111111001010011111011111111111100101110001011111111111000010011101111111111101110101001011111111100010110111111111111100011101110011111111110001111000111111111111001111111111\n",
      "Gamma decoded postings of 'instrument':\n",
      "{4226, 3079, 1679, 4367, 1811, 3349, 2456, 4891, 5920, 2086, 2477, 5168, 1841, 4657, 2486, 2234, 3005, 5438, 5573, 1102, 3922, 603, 2524, 1265, 3071}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "source": [
    "def encode_variable_byte(n):\r\n",
    "    encoded_num = []\r\n",
    "    binary_num = bin(n)[2:]\r\n",
    "    while True:\r\n",
    "        if len(binary_num) <= 7:\r\n",
    "            while len(binary_num)<7:\r\n",
    "                binary_num = '0' + binary_num\r\n",
    "            encoded_num.insert(0, binary_num)\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            last_byte = binary_num[-7:]\r\n",
    "            binary_num = binary_num[:-7]\r\n",
    "            encoded_num.insert(0, last_byte)\r\n",
    "\r\n",
    "    output_result = []\r\n",
    "    for i in range(len(encoded_num) - 1):\r\n",
    "        output_result.append('0' + encoded_num[i])\r\n",
    "    output_result.append('1' + encoded_num[-1])\r\n",
    "            \r\n",
    "    return ''.join(output_result)\r\n",
    "\r\n",
    "def byte_encode_inverted_index(inverted_index):\r\n",
    "    encoded_index = {}\r\n",
    "    for term, postings in inverted_index.items():\r\n",
    "        encoded_index[term] = ''.join([encode_variable_byte(doc_id) for doc_id in postings])\r\n",
    "    return encoded_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "source": [
    "def byte_decode_number(n):\r\n",
    "    true_binary_numbers = []\r\n",
    "    bytes_of_the_number = []\r\n",
    "    for i in range(0, len(n), 8):\r\n",
    "        byte = n[i:i+8]\r\n",
    "        bytes_of_the_number.append(byte)\r\n",
    "        if byte[0]=='1':\r\n",
    "            true_binary_num = ''\r\n",
    "            for b in bytes_of_the_number:\r\n",
    "                true_binary_num += b[1:]\r\n",
    "            true_binary_numbers.append(int(true_binary_num, 2))\r\n",
    "            bytes_of_the_number.clear()\r\n",
    "    return true_binary_numbers        \r\n",
    "\r\n",
    "def byte_decode_inverted_index(encoded_index):\r\n",
    "    decoded_index = {}\r\n",
    "    for term, postings in encoded_index.items():\r\n",
    "        decoded_index[term] = byte_decode_number(postings)\r\n",
    "    return decoded_index  \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "source": [
    "print(f\"\"\"original postings:\\n{final_index['instrument']}\"\"\")\r\n",
    "variable_byte_ecoded_postings = byte_encode_inverted_index(final_index)\r\n",
    "print(f\"\\nByte encoded postings of 'instrument': {variable_byte_ecoded_postings['instrument']}\")\r\n",
    "variable_byte_decoded_postings = byte_decode_inverted_index(variable_byte_ecoded_postings)\r\n",
    "print(f\"Byte dncoded postings of 'instrument':\\n{variable_byte_decoded_postings['instrument']}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original postings:\n",
      "{4226, 3079, 1679, 4367, 1811, 3349, 2456, 4891, 5920, 2086, 2477, 5168, 1841, 4657, 2486, 2234, 3005, 5438, 5573, 1102, 3922, 603, 2524, 1265, 3071}\n",
      "\n",
      "Byte encoded postings of 'instrument': 0010000110000010000110001000011100001101100011110010001010001111000011101001001100011010100101010001001110011000001001101001101100101110101000000001000010100110000100111010110100101000101100000000111010110001001001001011000100010011101101100001000110111010000101111011110100101010101111100010101111000101000010001100111000011110110100100000010011011011000100111101110000001001111100010001011111111111\n",
      "Byte dncoded postings of 'instrument':\n",
      "[4226, 3079, 1679, 4367, 1811, 3349, 2456, 4891, 5920, 2086, 2477, 5168, 1841, 4657, 2486, 2234, 3005, 5438, 5573, 1102, 3922, 603, 2524, 1265, 3071]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "source": [
    "import pickle\r\n",
    "def save_index(inverted_index, file_name):\r\n",
    "    with open(f'{file_name}.bin', 'wb') as f:\r\n",
    "        pickle.dump(inverted_index, f)\r\n",
    "\r\n",
    "def load_index(file_name):\r\n",
    "    with open(f'{file_name}.bin', 'rb') as f:\r\n",
    "        loaded_dict = pickle.load(f)\r\n",
    "    return loaded_dict    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "source": [
    "save_index(final_index, 'uncompressed_file')\r\n",
    "save_index(gamma_ecoded_postings, 'gamma_ecoded_postings')\r\n",
    "save_index(variable_byte_ecoded_postings, 'variable_byte_ecoded_postings')\r\n",
    "\r\n",
    "ind = load_index('uncompressed_file')\r\n",
    "print(ind['profit'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{3494, 4998, 5895, 4619, 142, 4977, 3443, 4118, 4348}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "source": [
    "import os\r\n",
    "def get_size(file_name):\r\n",
    "    file_size = os.path.getsize(file_name)\r\n",
    "    return file_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "source": [
    "print(f\"The size of the file is {get_size('uncompressed_file.bin')} bytes\")        \r\n",
    "print(f\"The size of the file is {get_size('gamma_ecoded_postings.bin')} bytes\")        \r\n",
    "print(f\"The size of the file is {get_size('variable_byte_ecoded_postings.bin')} bytes\")        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The size of the file is 1903546 bytes\n",
      "The size of the file is 10908426 bytes\n",
      "The size of the file is 7873443 bytes\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  },
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}